{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Model Selection and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In  this session, we will use the already seen datasets to illustrate important techniques like validation and regularization. First, we use the student \"exams\" dataset to understand the need of test data in addition to the training data.  \n",
    "\n",
    "Then, we will illustrate the under-fitting and over-fitting phenomenas on randomly generated data. After that, we will select a model that fits the best our validation data.\n",
    "\n",
    "Finally, we will implement regularization technique on the \"Microchip\" testing dataset. Thus, we can understand how this technique helps to prevent over-fitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preparation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics.regression import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as sklm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Train and Test data: Student \"exams\" dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will train a logistic classifier on the training data of student \"exams\" dataset. Then, we will predict the student admission of the test data and compare the accuracy of the classifier on the training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1: **  \n",
    "The *\"exams_train_data.txt\"* file contains 3 columns that represent the exam 1, exam 2 scores and the results of 100 students (0: Not admitted, 1: Admitted). \n",
    "- Load train data from \"exams_train_data.txt\" file in \"students_results_train\" variable and check its size. (use [loadtxt](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.loadtxt.html) function from numpy library)\n",
    "- Extract the number of students, the features and the output.\n",
    "- Implement the \"Poly_Features\" function that concatenates to data array the different possible powers (below deg) and interaction terms of feature vectors f1 and f2 as shown below:$$data=[1,~f_1,~ f_2,~ f_1^2,~ f_1\\times f_2,~ f_2^2,~ \\dots,~ f_1^{deg},~ f_1^{deg-1}\\times f_2,~\\dots,~ f_2^{deg}]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data contains 100 student results. There are 2 columns for each exam score and 1 column for admission\n",
      "(100, 1)\n",
      "(100, 1)\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "#load training data\n",
    "# Type your code here\n",
    "students_results_train =  np.loadtxt('exams_train_data.txt')\n",
    "\n",
    "# you could verify the size of the data using shape() function on numpy array house_data\n",
    "print(\"The training data contains {0} student results. There are {1} columns for each exam score and 1 column for admission\".format(students_results_train.shape[0],students_results_train.shape[1]-1))\n",
    "\n",
    "# Type your code here\n",
    "m_train =  students_results_train.shape[0]                                      # number of student\n",
    "x_1_train = students_results_train[:,0,np.newaxis]                                 # we add np.newaxis in the indexing to obtain an array \n",
    "x_2_train =  students_results_train[:,1,np.newaxis]                                     # with shape (m,1) instead of (m,)\n",
    "y_train =  students_results_train[:,2,np.newaxis]                                    # the student admission result vector\n",
    "print(x_1_train.shape)\n",
    "print(x_2_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate interaction terms (x_1^i*x_2^j where i+j<=degree)\n",
    "def Poly_Features(data,f1,f2,deg,ax=1):\n",
    "    # Type your code here\n",
    "    for i in range(1,deg+1): \n",
    "        for j in range(i+1):\n",
    "            terms= (f1**(i-j) * f2**j)\n",
    "            data = np.append(data,terms,ax)        \n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2: ** \n",
    "- Use the implemented function \"Poly_Features\" to generate the array of features X_train.\n",
    "- Determine the number of features n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features is:  6\n"
     ]
    }
   ],
   "source": [
    "# add polynomial features to the array data X\n",
    "degree=2  # degree of polynomial feature\n",
    "X_train=np.ones((m_train,1))   # initialize X array\n",
    "# Type your code here\n",
    "X_train = Poly_Features(X_train,x_1_train,x_2_train,degree,1)  \n",
    "n =X_train.shape[1]                        # number of features\n",
    "print(\"The number of features is: \",n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3: **  \n",
    "- Create a sigmoid function that returns $sigmoid(z)=\\frac{1}{1+e^{-z}}$  \n",
    "**Hint:** For a vectorized implementation:\n",
    "- Use [<code>numpy.ones</code>](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.ones.html) to have a numerator with the same shape of z\n",
    "- Use [<code>numpy.exp</code>](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # Type your code here\n",
    "    a1 = np.ones(z.shape)\n",
    "    sigmoid=a1/(a1+np.exp(-z))\n",
    "    return sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4: **\n",
    "- Create your logistic regression predict function\n",
    "$$yhat=sigmoid(\\theta^\\top x)=\\frac{1}{1+e^{-\\theta^\\top x}}$$\n",
    "**Hint:** Use the sigmoid function  \n",
    "This new hypothesis formulas will ensure: $0\\leq yhat_i\\leq 1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predict function\n",
    "def predict(x,theta):\n",
    "    # Type your code here\n",
    "    yhat=0\n",
    "    yhat = sigmoid(np.dot(x,theta))\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** \n",
    "- Store in the variable m the number of samples.\n",
    "- Define the criterion or cost function that returns the NLL (Negative Log Likelihood Error):  \n",
    "**Negative Log Likelihood Error (NLL)**: $$cost=J(\\theta)=\\frac{-1}{m}\\sum_{i=1}^{m}\\left [y\\times log(yhat)+(1-y)\\times log(1-yhat)\\right ]$$\n",
    "**Hint:** For a vectorized implementation:\n",
    "- Use [<code>numpy.sum</code>](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.sum.html) and [<code>numpy.log</code>](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.log.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL_cost(yhat, y):\n",
    "    # Type your code here\n",
    "    m=y.shape[0]\n",
    "    a1 = np.ones((y.shape[0],1))   \n",
    "    J =(-np.sum((np.log(yhat)*y)+(np.log(a1-yhat)*(a1-y))))/m\n",
    "    return J "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** \n",
    "The gradient vector of the logistic cost function is calculated as following: $$\\nabla J(\\theta) = \\begin{bmatrix}\\frac{\\partial J(\\theta)}{\\partial \\theta_0}\n",
    "\\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_1}\n",
    "\\\\ \\vdots\n",
    "\\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{n-1}}\n",
    "\\end{bmatrix}$$ \n",
    "where: $\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m}{(yhat_i - y_i)~x_{ij}} ~~for~ j=0\\dots n-1$\n",
    "- Implement the \"grad_cost_func\" function that evaluates the gradient of logistic cost function.  \n",
    "**Hint:** You can use the vectorized form: $\\nabla J(\\theta) =\\frac{1}{m} x^T(yhat-y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cost_func(theta, x, y):\n",
    "    # Type your code here\n",
    "    m = y.shape[0]\n",
    "    yhat=0\n",
    "    yhat=predict(x,theta)\n",
    "    g= np.dot(x.transpose(),(yhat-y))/m\n",
    "    return g  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7: **\n",
    "- Call \"fmin_bfgs\" function to calculate the optimal theta. This function take as parameters: the name of cost function (\"NLL_cost_fn\"), the name of gradient cost function (\"grad_cost_fn\") and the initial theta (\"theta0\").\n",
    "- Print the optimal theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL_cost_fn(theta):\n",
    "    J=NLL_cost(predict(X_train,theta[:,np.newaxis]), y_train)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cost_fn(theta):\n",
    "    g=grad_cost_func(theta[:,np.newaxis], X_train, y_train)\n",
    "    g.shape=(g.shape[0],)\n",
    "    return g  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 10\n",
      "         Function evaluations: 34\n",
      "         Gradient evaluations: 34\n",
      "The optimal value of theta that minimize cost function is:  [-0.39063862 -1.55957356 -1.90307948  0.01115259  0.26116427  0.02626949]\n"
     ]
    }
   ],
   "source": [
    "# calculate optimal theta\n",
    "theta0=np.zeros((n,))\n",
    "#theta0 = np.zeros(shape=(X_train.shape[1], 1))\n",
    "print(theta0.shape)\n",
    "# Type your code here\n",
    "Thopt,_,_,_,_,_,_, =  fmin_bfgs(NLL_cost_fn, theta0, grad_cost_fn, full_output=True)\n",
    "print(\"The optimal value of theta that minimize cost function is: \",Thopt)\n",
    "#print(\"Final error = \",NLL_cost(predict(X_train,Thopt[:,np.newaxis]),y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8: **\n",
    "- Predict \"y_pred_train\" the admission result of each student on train data.\n",
    "- Calculate the training accuracy (number of good prediction/number of all student) on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100,)\n",
      "[3.17040596e-03 3.64205725e-05 2.84224355e-03 9.99991708e-01\n",
      " 9.99999968e-01 5.92024295e-03 9.99999953e-01 4.39288296e-01\n",
      " 1.00000000e+00 5.71297929e-01 2.86889180e-01 3.47997527e-04\n",
      " 9.99999997e-01 1.00000000e+00 2.99838365e-02 9.99843482e-01\n",
      " 7.49408561e-01 1.34833439e-01 1.00000000e+00 6.87648123e-01\n",
      " 2.64335035e-02 9.99999923e-01 4.93153184e-03 7.37125492e-05\n",
      " 9.99997454e-01 9.97341031e-01 5.13655166e-01 2.66331070e-01\n",
      " 1.21197141e-01 3.09279065e-03 9.98764514e-01 9.99853394e-01\n",
      " 2.87914861e-01 7.27160304e-01 1.68375055e-02 3.52932941e-02\n",
      " 5.72357384e-02 9.99992233e-01 5.95749135e-02 1.75626445e-03\n",
      " 9.99601345e-01 3.75785003e-03 9.99999989e-01 1.36422649e-01\n",
      " 2.58789260e-03 2.29836539e-01 9.99998493e-01 1.00000000e+00\n",
      " 9.99999979e-01 1.00000000e+00 9.99999959e-01 9.99999958e-01\n",
      " 8.23081396e-01 3.32697509e-04 5.87049458e-03 5.83070143e-02\n",
      " 1.00000000e+00 1.32421879e-02 9.99990857e-01 9.99999683e-01\n",
      " 9.99999999e-01 1.13770744e-04 1.13424462e-03 4.17445470e-05\n",
      " 4.37105542e-02 1.13443554e-02 8.59792523e-01 7.05762497e-03\n",
      " 1.00000000e+00 9.82100205e-01 5.36807155e-05 9.99968919e-01\n",
      " 1.00000000e+00 9.98236176e-01 9.98648181e-01 1.00000000e+00\n",
      " 9.83640487e-01 8.95129499e-01 6.33451098e-03 3.37381143e-01\n",
      " 9.99999993e-01 9.84018380e-01 9.98857306e-01 4.68215934e-01\n",
      " 1.00000000e+00 9.99999943e-01 1.77728167e-01 1.00000000e+00\n",
      " 1.00000000e+00 1.79693397e-01 1.00000000e+00 1.00000000e+00\n",
      " 3.63500689e-04 9.99999998e-01 9.34132430e-01 9.49468527e-01\n",
      " 6.20202861e-01 1.00000000e+00 7.26816367e-01 1.00000000e+00]\n",
      "y_good [False False False  True  True False  True False  True  True False False\n",
      "  True  True False  True  True False  True  True False  True False False\n",
      "  True  True  True False False False  True  True False  True False False\n",
      " False  True False False  True False  True False False False  True  True\n",
      "  True  True  True  True  True False False False  True False  True  True\n",
      "  True False False False False False  True False  True  True False  True\n",
      "  True  True  True  True  True  True False False  True  True  True False\n",
      "  True  True False  True  True False  True  True False  True  True  True\n",
      "  True  True  True  True]\n",
      "count 58\n",
      "good_pre--> 58\n",
      "The accuracy on the training data is: 57.99999999999999 %\n"
     ]
    }
   ],
   "source": [
    "# calculate prediction and train accuracy\n",
    "# Type your code here\n",
    "y_pred_train = predict(X_train,Thopt)\n",
    "y_good =  y_pred_train >=0.5\n",
    "print(y_pred_train.shape)\n",
    "print(y_good.shape)\n",
    "count=0\n",
    "for i in range (1,y_pred_train.shape[0]):\n",
    "    if y_pred_train[i]> 0.5: \n",
    "        y_good[i]=1\n",
    "        count+=1\n",
    "   # else: \n",
    "     #   y_good[i]= 0\n",
    "print(y_pred_train)\n",
    "print(\"y_good\",y_good)\n",
    "print(\"count\",count)\n",
    "good_pre = np.count_nonzero(y_good)\n",
    "print(\"good_pre-->\",good_pre)\n",
    "train_accuracy = good_pre / X_train.shape[0] \n",
    "print(\"The accuracy on the training data is:\", train_accuracy*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the decision boundaries on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100,)\n",
      "(100, 100, 1)\n",
      "(100, 100, 6)\n",
      "(6,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x6b49550>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate the mesh grid for contour plot\n",
    "u1=np.linspace(5,20,100)\n",
    "u2=np.linspace(5,20,100)\n",
    "print(u1.shape)\n",
    "print(u2.shape)\n",
    "u1, u2 = np.meshgrid(u1, u2)\n",
    "\n",
    "X3=np.ones((*u1.shape,1))\n",
    "print(X3.shape)\n",
    "\n",
    "X3=Poly_Features(X3,u1[...,np.newaxis],u2[...,np.newaxis],degree,-1)\n",
    "\n",
    "Z=np.dot(X3,Thopt)\n",
    "\n",
    "# X3=Poly_Features(X3,u1,u2,degree,-1)\n",
    "print(X3.shape)\n",
    "print(Thopt.shape)\n",
    "Z=np.dot(X3,Thopt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot descision boundries\n",
    "plt.figure(\"Admission decision boundries\",figsize=(9,5))\n",
    "fail=plt.scatter(x_1_train[y_train==0], x_2_train[y_train==0],  color='red',label='fail')\n",
    "success=plt.scatter(x_1_train[y_train==1], x_2_train[y_train==1],  color='green',marker='+',s=80,label='success')\n",
    "plt.xlabel('Exam 1 score')\n",
    "plt.ylabel('Exam 2 score')\n",
    "plt.title('Adimitted/Not admitted Students')\n",
    "ctr = plt.contour(u1, u2, Z,0,colors=\"blue\")\n",
    "extra = Rectangle((0, 0), 3, 4, fc=\"w\", fill=False, edgecolor=\"b\", linewidth=1)\n",
    "plt.legend([extra,fail,success], (\"decision boundries\",\"fail\",\"success\"),loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9: **\n",
    "- Load test data from \"exams_test_data.txt\" file in \"students_results_test\" variable and check its size. (use [loadtxt](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.loadtxt.html) function from numpy library).\n",
    "- Calculate the test accuracy (number of good prediction/number of all student) on the test data and compare it with the train accuracy. Interpret the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here\n",
    "students_results_test = np.loadtxt('exams_test_data.txt',delimiter='\\t')  # ** your code here** \n",
    "\n",
    "# Type your code here\n",
    "m_test =   students_results_test.shape[0]                         # number of student in test data\n",
    "x_1_test =  students_results_test[:,0,np.newaxis]      # we add np.newaxis in the indexing to obtain an array \n",
    "x_2_test =   students_results_test[:,1,np.newaxis]     # with shape (m,1) instead of (m,)\n",
    "y_test =  students_results_test[:,2,np.newaxis]        # we add np.newaxis in the indexing to obtain an array with shape (100,1) instead of (100,)\n",
    "\n",
    "# add polynomial features to the array data X_test\n",
    "# Type your code here\n",
    "X_test=np.ones((m_test,1))   # initialize X array\n",
    "X_test = Poly_Features(X_test,x_1_train,x_2_train,degree,1) \n",
    "\n",
    "# calculate prediction and accuracy on test data\n",
    "# Type your code here\n",
    "y_test_pred = \n",
    "test_accuracy =  \n",
    "\n",
    "y_test_pred = predict(X_test,Thopt)\n",
    "y_goodtest =  y_test_pred >=0.5\n",
    "print(y_test_pred.shape)\n",
    "print(y_goodtest.shape)\n",
    "count=0\n",
    "for i in range (1,y_test_pred.shape[0]):\n",
    "    if y_test_pred[i]> 0.5: \n",
    "        y_goodtest[i]=1\n",
    "        count+=1\n",
    "   # else: \n",
    "     #   y_good[i]= 0\n",
    "print(y_test_pred)\n",
    "print(\"y_good\",y_goodtest)\n",
    "print(\"count\",count)\n",
    "good_pre = np.count_nonzero(y_goodtest)\n",
    "print(\"good_pre-->\",y_goodtest)\n",
    "test_accuracy = y_goodtest / X_test.shape[0] \n",
    "print(\"The accuracy on the test data is:\", test_accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the decision boundaries on the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot descision boundries and test data\n",
    "plt.figure(\"decision boundries and test data\",figsize=(9,5))\n",
    "fail=plt.scatter(x_1_test[y_test==0], x_2_test[y_test==0],  color='red')\n",
    "success=plt.scatter(x_1_test[y_test==1], x_2_test[y_test==1],  color='green',marker='+',s=80)\n",
    "fail_train=plt.scatter(x_1_train[y_train==0], x_2_train[y_train==0],  color='gray')\n",
    "success_train=plt.scatter(x_1_train[y_train==1], x_2_train[y_train==1],  color='gray',marker='+',s=80)\n",
    "plt.xlabel('Exam 1 score')\n",
    "plt.ylabel('Exam 2 score')\n",
    "plt.title('Adimitted/Not admitted Students')\n",
    "ctr = plt.contour(u1, u2, Z,0,colors=\"blue\")\n",
    "plt.legend([extra,fail,success,fail_train,success_train], (\"decision boundries\",\"test data (fail)\",\"test data (success)\",\"train data (fail)\",\"train data (success)\"),loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Underfitting and Overfitting\n",
    "\n",
    "In this part we will study the effect of the number of features and the model complexity on the training phase and we will illustrate the underfitting and overfitting phenomena. We will use randomly generated data for a regression problem and we will try to use several models with different number of features (different polynomial features degrees). Then, we will see how well the model fits the training and the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1: **\n",
    "- Split the data (\"x\" and \"y\" vector) to training and test data with size \"m_train\" and \"m_test\" respectively. The original data is sorted. Therefore, you should choose the training and the test sets randomly to ensure that the two sets cover the possible values space in a best manner.  \n",
    "**Hint:** You can use [random permutation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.permutation.html) function to generate a random permutation of \"m\" indices for \"x\" and \"y\" vectors. Then, you can select the first \"m_train\" indices to index train data from \"x\" and \"y\". You can use the rest of indices to index the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random data\n",
    "np.random.seed(0)\n",
    "m = 15\n",
    "x = np.linspace(0,10,m) + np.random.randn(m)/5\n",
    "y = np.sin(x)+x/6 + np.random.randn(m)/10\n",
    "\n",
    "# calculate the size of training and test sets\n",
    "train_ratio = 0.75\n",
    "m_train = int(round(train_ratio*m)) \n",
    "m_test = m-m_train\n",
    "print(\"the size of training set is:\",m_train,\"\\nthe size of test set is:\",m_test)\n",
    "\n",
    "# split the data to training and test sets\n",
    "np.random.seed(9599)\n",
    "# Type your code here\n",
    "rand_perm = \n",
    "X_train=\n",
    "y_train=\n",
    "X_test=\n",
    "y_test="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize training and test set\n",
    "plt.figure(\"training and test set\")\n",
    "plt.scatter(X_train, y_train, label='training data')\n",
    "plt.scatter(X_test, y_test, label='test data')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2: **\n",
    "- Use [PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) class and [PolynomialFeatures.fit_transform](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures.fit_transform) functions from numpy library to generate polynomial features of degree \"i\". This will generate automatically the features instead of adding them by hand as in the previous example with the implemented function \"Poly_Features\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train several polynomial models\n",
    "regr = LinearRegression()\n",
    "deg=[1,3,6,9,11]\n",
    "Poly_predict=np.zeros((len(deg),200))\n",
    "for i in range(len(deg)):\n",
    "    # Type your code here\n",
    "    poly = \n",
    "    new_X =  \n",
    "    regr.fit(new_X, y_train[:,np.newaxis])\n",
    "    u = np.linspace(0,11.5,200)   # \n",
    "    new_U = poly.fit_transform(u[:,np.newaxis])\n",
    "    Poly_predict[i,:]=regr.predict(new_U).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize different polynomial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize different polynomial models\n",
    "plt.figure(\"different polynomial models\",figsize=(9,5))\n",
    "plt.plot(X_train, y_train, 'o', label='training data', markersize=10)\n",
    "plt.plot(X_test, y_test, 'o', label='test data', markersize=10)\n",
    "for i,degree in enumerate(deg):\n",
    "    plt.plot(u, Poly_predict[i], alpha=0.8, lw=2, label='degree={}'.format(degree))\n",
    "plt.ylim(-1.5,4)\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "In this part, we will train several polynomial models. Then, we will assess their performance on the training and the test sets. Hence, we will choose  the model with best performance in both training and test set. However, the calculated performance on test data won't be a good estimation of the performance of our model in general case. In fact, our model order (polynomial degree) is fitted to the test data. Thus, it tends to perform better on test data than on a new data. Therefore, we introduce the validation data used for tuning model meta-parameters (polynomial degree, classification threshold...). Then, we will use the test data to estimate the performance of our model in general case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3: **\n",
    "- Split the original data (\"x\" and \"y\" vector) to training, validation and test data with size \"m_train\", \"m_val\" and \"m_test\" respectively.  \n",
    "**Hint:** You can use [random permutation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.permutation.html) function to generate a random permutation of \"m\" indices for \"x\" and \"y\" vectors. Then, you can select the first \"m_train\" indices to index train data from \"x\" and \"y\". Then, you can act similarly for validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the size of training, validation and test sets\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "m_train = int(round(train_ratio*m)) \n",
    "m_val = int(round(val_ratio*m)) \n",
    "m_test = m-m_train-m_val\n",
    "print(\"the size of training set is:\",m_train,\"\\nthe size of validation set is:\",m_val,\"\\nthe size of test set is:\",m_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to training, validation and test sets\n",
    "np.random.seed(5190969)\n",
    "# Type your code here\n",
    "rand_perm = \n",
    "X_train=\n",
    "y_train=\n",
    "X_val=\n",
    "y_val=\n",
    "X_test=\n",
    "y_test="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize training, validation and test sets\n",
    "plt.figure(\"training, validation and test sets\")\n",
    "plt.scatter(X_train, y_train, label='training data')\n",
    "plt.scatter(X_val, y_val, label='validation data')\n",
    "plt.scatter(X_test, y_test, label='test data')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4: **\n",
    "- Calculate \"train_error\" and \"val_error\" mean squared error on training and validation set for each polynomial model with degree \"i\" (for loop counter).  \n",
    "**Hint:** You can use [mean_squared_error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) function from sklearn library to evaluate the mean squared error between original \"y\" and \"y_predicted\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and assess several polynomial models\n",
    "train_error = np.zeros((deg[-1]+1,)) \n",
    "val_error = np.zeros((deg[-1]+1,))\n",
    "\n",
    "for i in range(deg[-1]+1):\n",
    "    poly = PolynomialFeatures(i)\n",
    "    new_X_train= poly.fit_transform(X_train[:,np.newaxis])\n",
    "    new_X_val = poly.fit_transform(X_val[:,np.newaxis])\n",
    "    regr.fit(new_X_train, y_train[:,np.newaxis])\n",
    "    # Type your code here\n",
    "    y_train_predicted = \n",
    "    y_val_predicted = \n",
    "    train_error[i] = \n",
    "    val_error[i] = \n",
    "    print(\"degree {} polynomial has a train error: {:.5f} and validation error: {:.4f}\".format(i,train_error[i],val_error[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize error of each polynomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize error of each polynomial model\n",
    "plt.figure(\"train and validation error of each polynomial model\")  \n",
    "plt.plot(range(deg[-1]+1),train_error[:deg[-1]+1],label=\"train error\")\n",
    "plt.plot(range(deg[-1]+1),val_error[:deg[-1]+1],label=\"validation error\")\n",
    "plt.ylim(0,1)\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5: **\n",
    "- From the train and validation error graph and values, select the polynomial degree \"best_poly_deg\" that fit the best our data. Then, compare the training, validation and test error of this polynomial model. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here\n",
    "best_poly_deg = \n",
    "poly = PolynomialFeatures(best_poly_deg)\n",
    "new_X_train= \n",
    "new_X_test = \n",
    "regr.fit(new_X_train, y_train[:,np.newaxis])\n",
    "# Type your code here\n",
    "y_test_predicted = \n",
    "test_error = \n",
    "print(\"train error =\",train_error[best_poly_deg],\"\\nvalidation error =\",val_error[best_poly_deg],\"\\ntest error =\",test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Regularization\n",
    "\n",
    "In this section, we will split the \"Microchip\" dataset into training and test sets. We will train a polynomial logistic classifier with and without regularization. Then, we will compare train and test accuracy in the two cases.  \n",
    "\n",
    "The regularization helps to avoid the problem of overfitting by reducing or even making null $\\theta_j's$ of non significant features. The idea is to include the sum of $\\theta_j's$ in cost function in order to reduce them and remove useless features in our models.\n",
    "\n",
    "The regularized cost function is equal to: $$J_{Reg}(\\theta)= J(\\theta)+\\frac{\\lambda}{2m}\\times\\sum_{j=1}^{n-1} \\theta_j^2$$\n",
    "\n",
    "The gradient of regularized cost function is equal to: \n",
    "\n",
    "$$\\nabla J_{Reg}(\\theta) = \\begin{bmatrix}\\frac{\\partial J(\\theta)}{\\partial \\theta_0}\n",
    "\\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_1}\n",
    "\\\\ \\vdots\n",
    "\\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{n-1}}\n",
    "\\end{bmatrix}+\\frac{\\lambda}{m} \\begin{bmatrix}0\n",
    "\\\\ \\theta_1\n",
    "\\\\ \\vdots\n",
    "\\\\ \\theta_{n-1}\n",
    "\\end{bmatrix}$$ \n",
    "where: $ \\left\\{\\begin{matrix}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m}{(h_\\theta(x_i) - y)~x_j} ~~for~ j=0\\dots n-1 \n",
    "\\\\ h_\\theta(x_i)=sigmoid(\\theta^\\top x_i)=\\frac{1}{1+e^{-\\theta^\\top x_i}}\n",
    "\\end{matrix}\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1: **\n",
    "- Load data from \"microchip.txt\" file (use [loadtxt](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.loadtxt.html) function from numpy library).\n",
    "- Split \"X\" and \"Y\" data into train, validation and test sets with a size of 60%, 20% and 20% respectively.  \n",
    "**Hint: ** You can use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from sklearn library (use the parameter random_state=799) instead of generating random permutation by hand.\n",
    "- Extract the train features and use the implemented function \"Poly_Features\" to generate the array of train features X.\n",
    "- Determine the number of features n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and extract data\n",
    "# Type your code here\n",
    "microchip_data =                    \n",
    "m =                                # number of mocrochips\n",
    "x_1 =                               # we add np.newaxis in the indexing to obtain an array \n",
    "x_2 =                                # with shape (m,1) instead of (m,)\n",
    "Y =                                   # we add np.newaxis in the indexing to obtain an array with shape (m,1) instead of (m,)\n",
    "\n",
    "# split data to train, validation and test sets use the parameter random_state=799\n",
    "# Type your code here\n",
    "X_train, X_val, y_train, y_val = \n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add polynomial features to the train data\n",
    "degree=6    # degree of polynomial features\n",
    "# Type your code here\n",
    "x_1_train = \n",
    "x_2_train = \n",
    "Y=\n",
    "\n",
    "# Type your code here\n",
    "X =    # initialize X array\n",
    "X = \n",
    "n = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2: **  \n",
    "- Create a sigmoid function that returns $sigmoid(z)=\\frac{1}{1+e^{-z}}$  \n",
    "**Hint:** For a vectorized implementation:\n",
    "- Use [<code>numpy.ones</code>](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.ones.html) to have a numerator with the same shape of z\n",
    "- Use [<code>numpy.exp</code>](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # Type your code here\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3: **\n",
    "- Create your logistic regression predict function\n",
    "$$yhat=sigmoid(\\theta^\\top x)=\\frac{1}{1+e^{-\\theta^\\top x}}$$\n",
    "**Hint:** Use the sigmoid function  \n",
    "This new hypothesis formulas will ensure: $0\\leq yhat_i\\leq 1$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predict function\n",
    "def predict(x,theta):\n",
    "    # Type your code here\n",
    "    yhat = \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** \n",
    "- Store in the variable m the number of samples.\n",
    "- Define the **regulized** cost function that returns the NLL (Negative Log Likelihood Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reg_NLL_cost(theta, x, y ,lambda_):\n",
    "    # ** Type your code here** \n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** \n",
    "- Implement the \"Reg_NLL_cost\" function that evaluates the gradient of the **regulized** logistic cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reg_grad_cost(theta, x, y ,lambda_):\n",
    "    # ** Type your code here** \n",
    "    \n",
    "    return  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6: **\n",
    "- Extract the validation features and use the implemented function \"Poly_Features\" to generate the array of validation features X_val_poly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add polynomial features to the test data  \n",
    "# ** Type your code here**\n",
    "x_1_val =  \n",
    "x_2_val = \n",
    "X_val_poly=np.ones((X_val.shape[0],1))   # initialize X_test_poly array\n",
    "X_val_poly = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7: **\n",
    "- Calculate the values of train and validation accuracies in the case with the different values values of lambda.  \n",
    "**Hint:** You can use the function [<code>accuracy_score</code>](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reg_NLL_cost_fn(theta):\n",
    "    J=Reg_NLL_cost(theta[:,np.newaxis],X, Y, lambda_)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reg_grad_cost_fn(theta):\n",
    "    g=Reg_grad_cost(theta[:,np.newaxis], X, Y, lambda_)\n",
    "    g.shape=(g.shape[0],)\n",
    "    return g  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = np.arange(0.0,5.0,0.01)\n",
    "train_Acc = []\n",
    "val_Acc = []\n",
    "theta0=np.zeros((n,))\n",
    "for reg in l2:\n",
    "    lambda_=reg\n",
    "    theta_opt = fmin_bfgs(Reg_NLL_cost_fn,theta0,fprime=Reg_grad_cost_fn,disp=0) \n",
    "    # ** your code here**\n",
    "    y_train_pred =  \n",
    "    train_accuracy=\n",
    "    train_Acc.append(train_accuracy)\n",
    "    # ** your code here**\n",
    "    y_val_pred = \n",
    "    val_accuracy=\n",
    "    val_Acc.append(val_accuracy)\n",
    "max_idx = np.argmax(val_Acc)\n",
    "max_l2 = l2[max_idx]\n",
    "# ** your code here**\n",
    "max_Acc = val_Acc[max_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize train and validation accuracy variations based on the regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"Train and validation accuracy \\n vs. regularization parameter\")\n",
    "plt.plot(l2, val_Acc, color = 'red', label = 'Val Acc')\n",
    "plt.plot(l2, train_Acc, label = 'Train Acc')\n",
    "plt.axvline(max_l2, color = 'black', linestyle = '--')\n",
    "plt.legend()\n",
    "plt.xlabel('Regularization parameter')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8: **\n",
    "- Extract the test features and use the implemented function \"Poly_Features\" to generate the array of test features X_test_poly.\n",
    "- Calculate the test accuracy with the value of lambda that maximizes the validation accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** your code here**\n",
    "x_1_test =  \n",
    "x_2_test = \n",
    "X_test_poly=np.ones((X_test.shape[0],1))   # initialize X_test_poly array\n",
    "X_test_poly = \n",
    "\n",
    "lambda_=\n",
    "theta_opt = \n",
    "\n",
    "y_test_pred = \n",
    "test_accuracy=\n",
    "print ('The test accuracy is ', test_accuracy*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mesh grid for contour plot\n",
    "u1=np.linspace(-1,1.5,50)\n",
    "u2=np.linspace(-1,1.5,50)\n",
    "u1, u2 = np.meshgrid(u1, u2)\n",
    "X3=np.ones((*u1.shape,1))\n",
    "\n",
    "X3=Poly_Features(X3,u1[...,np.newaxis],u2[...,np.newaxis],degree,-1)\n",
    "\n",
    "Z=np.dot(X3,theta_opt)\n",
    "\n",
    "# plot descision boundries\n",
    "plt.figure(\"Microchip decision boundries\",figsize=(9,7))\n",
    "fail=plt.scatter(x_1_test[y_test==0], x_2_test[y_test==0],  color='red',label='fail')\n",
    "success=plt.scatter(x_1_test[y_test==1], x_2_test[y_test==1],  color='green',marker='+',s=80,label='success')\n",
    "\n",
    "fail_train=plt.scatter(x_1_train[y_train==0], x_2_train[y_train==0],  color='gray',label='fail')\n",
    "success_train=plt.scatter(x_1_train[y_train==1], x_2_train[y_train==1],  color='gray',marker='+',s=80,label='success')\n",
    "\n",
    "fail_val=plt.scatter(x_1_val[y_val==0], x_2_val[y_val==0],  color='orange',label='fail')\n",
    "success_val=plt.scatter(x_1_val[y_val==1], x_2_val[y_val==1],  color='blue',marker='+',s=80,label='success')\n",
    "\n",
    "plt.xlabel('Test 1 score')\n",
    "plt.ylabel('Test 2 score')\n",
    "plt.title('Accepted/Rejected Microchip')\n",
    "ctr = plt.contour(u1, u2, Z,0,colors=\"blue\")\n",
    "plt.legend([extra,fail,success,fail_train,success_train,fail_val,success_val], (\"decision boundries\",\"test data (fail)\",\"test data (success)\",\"train data (fail)\",\"train data (success)\",\"validation data (fail)\",\"validation data (success)\"),loc='best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
