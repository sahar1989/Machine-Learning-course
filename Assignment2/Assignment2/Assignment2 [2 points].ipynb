{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Two-layer neural network\n",
    "\n",
    "In this assignment, you will learn how to:\n",
    "- Build a 2-class classification neural network with a single hidden layer\n",
    "- Compute the cross-entropy loss \n",
    "- Implement forward and backward propagation\n",
    "\n",
    "Let's first import all the packages that you will need during this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Let's get the dataset you will work on. The following code will load a \"flower\" 2-class dataset into variables `X` and `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0ef3ba6a6d41d7a30720d29b211d726f",
     "grade": false,
     "grade_id": "flower_dataset",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def load_planar_dataset():\n",
    "    np.random.seed(1)\n",
    "    m = 400 # number of examples\n",
    "    N = int(m/2) # number of points per class\n",
    "    D = 2 # dimensionality\n",
    "    X = np.zeros((m,D)) # data matrix where each row is a single example\n",
    "    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n",
    "    a = 4 # maximum ray of the flower\n",
    "\n",
    "    for j in range(2):\n",
    "        ix = range(N*j,N*(j+1))\n",
    "        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
    "        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        Y[ix] = j\n",
    "        \n",
    "    X = X.T\n",
    "    Y = Y.T\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "X, Y = load_planar_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data. They look like a \"flower\" with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data:\n",
    "plt.scatter(X[0, :], X[1, :], c=Y.flatten(), s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have:\n",
    "- a numpy-array (matrix) X that contains your features (x1, x2)\n",
    "- a numpy-array (vector) Y that contains your labels (red:0, blue:1).\n",
    "\n",
    "Lets first get a better sense of what our data is like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise**: How many training examples do you have? In addition, what is the `shape` of the variables `X` and `Y`? \n",
    "\n",
    "**Hint**: How do you get the shape of a numpy array? [(help)](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5801a7d6134e0fc81a22ae2d6bd31774",
     "grade": false,
     "grade_id": "flower_shape",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "shape_X = None\n",
    "shape_Y = None\n",
    "N       = None  # training set size\n",
    "### END CODE HERE ###\n",
    "\n",
    "print ('The shape of X is: ' + str(shape_X))\n",
    "print ('The shape of Y is: ' + str(shape_Y))\n",
    "print ('I have N = %d training examples!' % (N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "       \n",
    "<table style=\"width:20%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td>**shape of X**</td>\n",
    "    <td> (2, 400) </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**shape of Y**</td>\n",
    "    <td>(1, 400) </td> \n",
    "  </tr>\n",
    "  \n",
    "    <tr>\n",
    "    <td>**N**</td>\n",
    "    <td> 400 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f109e3ea22a225f357a54b3b41222fb7",
     "grade": true,
     "grade_id": "flower_shape_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert shape_X == (2,400)\n",
    "assert shape_Y == (1,400)\n",
    "assert N == 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network model\n",
    "\n",
    "Now, you are going to train a Neural Network with a single hidden layer. The general methodology to build a Neural Network is to:\n",
    " 1. Define the neural network structure (number of input units, number of hidden units, etc). \n",
    " 2. Initialize the model's parameters\n",
    " 3. Loop:\n",
    "    - Perform forward propagation\n",
    "    - Compute the loss function\n",
    "    - Perform backward propagation to get the gradients\n",
    "    - Update the parameters (one iteration of gradient descent)\n",
    "\n",
    "You will build helper functions to compute steps 1-3, and then merge them into one function `nn_model()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Two-layer network for binary classification**\n",
    "\n",
    "For each input vector ${\\rm x}$, the network performs the followin chain of operations.\n",
    "\n",
    "- The hidden layer transform the input's network into a vector of size $M_h$\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\\\\n",
    "{\\rm z}^{[1]} &=  W^{[1]} {\\rm x} + {\\rm b}^{[1]}\\\\ \n",
    "{\\rm a}^{[1]} &= \\tanh\\big({\\rm z}^{[1]}\\big)\\\\\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "where the matrix $W^{[1]}$ and the column vector ${\\rm b}^{[1]}$ gather the hidden layer's parameters\n",
    "$$\n",
    "W^{[1]} = \n",
    "\\begin{bmatrix}\n",
    "\\_\\!\\_\\; {\\rm w}_1^\\top \\_\\!\\_ \\\\\n",
    "\\vdots\\\\\n",
    "\\_\\!\\_\\; {\\rm w}_{M_1}^\\top \\_\\!\\_ \\\\\n",
    "\\end{bmatrix}\n",
    "\\qquad\\qquad\n",
    "{\\rm b}^{[1]} = \n",
    "\\begin{bmatrix} \n",
    "b_1 \\\\\n",
    "\\vdots\\\\\n",
    "b_{M_1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "- The output layer transforms the hidden representation into the network's output\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\\\\n",
    "{\\rm z}^{[2]} &=  W^{[2]} {\\rm a}^{[1]} + {\\rm b}^{[2]}\\\\ \n",
    "{\\rm a}^{[2]} &= \\sigma\\big({\\rm z}^{[2]}\\big)\\\\\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "where the matrix $W^{[2]}$ and the column vector ${\\rm b}^{[2]}$ gather the output layer's parameters\n",
    "$$\n",
    "W^{[2]} = \n",
    "\\begin{bmatrix}\n",
    "\\_\\!\\_\\; {\\rm w}_1^\\top \\_\\!\\_ \\\\\n",
    "\\vdots\\\\\n",
    "\\_\\!\\_\\; {\\rm w}_{M_2}^\\top \\_\\!\\_ \\\\\n",
    "\\end{bmatrix}\n",
    "\\qquad\\qquad\n",
    "{\\rm b}^{[2]} = \n",
    "\\begin{bmatrix} \n",
    "b_1 \\\\\n",
    "\\vdots\\\\\n",
    "b_{M_2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In our specific case of binary classification, the network's output is a scalar. Hence, $M_2=1$ and $\\sigma$ denotes the sigmoid function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Defining the neural network structure ####\n",
    "\n",
    "**Exercise**: Define three variables:\n",
    " - n_x: the size of the input layer\n",
    " - n_h: the size of the hidden layer (set this to 4) \n",
    " - n_y: the size of the output layer\n",
    "\n",
    "**Hint**: Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a2c9407fb3bac8b2e265e6475dfb69ad",
     "grade": false,
     "grade_id": "layer_sizes",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: layer_sizes\n",
    "\n",
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    n_x = None # size of input layer\n",
    "    n_h = None\n",
    "    n_y = None # size of output layer\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "130437de7e2d53f0a653a705c2181c39",
     "grade": false,
     "grade_id": "layer_sizes_assess",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_assess = np.random.randn(5, 3)\n",
    "Y_assess = np.random.randn(2, 3)\n",
    "    \n",
    "(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)\n",
    "\n",
    "print(\"The size of the input  layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output** (these are not the sizes you will use for your network, they are just used to assess the function you've just coded).\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "  <tr>\n",
    "    <td>**n_x**</td>\n",
    "    <td> 5 </td> \n",
    "  </tr>\n",
    "  \n",
    "    <tr>\n",
    "    <td>**n_h**</td>\n",
    "    <td> 4 </td> \n",
    "  </tr>\n",
    "  \n",
    "    <tr>\n",
    "    <td>**n_y**</td>\n",
    "    <td> 2 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3c1a669bfc1b95d59be5061f0d99f2fe",
     "grade": true,
     "grade_id": "layer_sizes_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert n_x == 5\n",
    "assert n_h == 4\n",
    "assert n_y == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Initialize the model's parameters ####\n",
    "\n",
    "**Exercise**: Implement the function `initialize_parameters()`.\n",
    "\n",
    "**Instructions**:\n",
    "- Make sure your parameters' sizes are right.\n",
    "- You will initialize the weights matrices with random values. \n",
    "    - Use: `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b).\n",
    "- You will initialize the bias vectors as zeros. \n",
    "    - Use: `np.zeros((a,b))` to initialize a matrix of shape (a,b) with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f916b612d6f10586bcc0a1bd137bd5a7",
     "grade": false,
     "grade_id": "initialize_parameters",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d5d23c12651f445725fd3425614c339",
     "grade": false,
     "grade_id": "initialize_parameters_assess",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n_x, n_h, n_y = 2, 4, 1\n",
    "\n",
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "print(\"W1 = \\n\" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \\n\" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \\n\" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \\n\" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:90%\">\n",
    "  <tr>\n",
    "    <td>**W1**</td>\n",
    "    <td> [[-0.00416758 -0.00056267]\n",
    " [-0.02136196  0.01640271]\n",
    " [-0.01793436 -0.00841747]\n",
    " [ 0.00502881 -0.01245288]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1**</td>\n",
    "    <td> [[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[-0.01057952 -0.00909008  0.00551454  0.02292208]]</td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  <tr>\n",
    "    <td>**b2**</td>\n",
    "    <td> [[ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3d3cde95fbe8b8b27a242bbc27730818",
     "grade": true,
     "grade_id": "initialize_parameters_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "W1 = parameters[\"W1\"]\n",
    "np.testing.assert_almost_equal(W1[0,0], -0.00416758)\n",
    "np.testing.assert_almost_equal(W1[0,1], -0.00056267)\n",
    "np.testing.assert_almost_equal(W1[1,0], -0.02136196)\n",
    "np.testing.assert_almost_equal(W1[1,1],  0.01640271)\n",
    "np.testing.assert_almost_equal(W1[2,0], -0.01793436)\n",
    "np.testing.assert_almost_equal(W1[2,1], -0.00841747)\n",
    "np.testing.assert_almost_equal(W1[3,0],  0.00502881)\n",
    "np.testing.assert_almost_equal(W1[3,1], -0.01245288)\n",
    "\n",
    "b1 = parameters[\"b1\"]\n",
    "np.testing.assert_almost_equal(b1[0,0], 0.0)\n",
    "np.testing.assert_almost_equal(b1[1,0], 0.0)\n",
    "np.testing.assert_almost_equal(b1[2,0], 0.0)\n",
    "np.testing.assert_almost_equal(b1[3,0], 0.0)\n",
    "\n",
    "W2 = parameters[\"W2\"]\n",
    "np.testing.assert_almost_equal(W2[0,0], -0.01057952)\n",
    "np.testing.assert_almost_equal(W2[0,1], -0.00909008)\n",
    "np.testing.assert_almost_equal(W2[0,2],  0.00551454)\n",
    "np.testing.assert_almost_equal(W2[0,3],  0.02292208)\n",
    "\n",
    "b2 = parameters[\"b2\"]\n",
    "np.testing.assert_almost_equal(b2[0,0], 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Forward propagation \n",
    "\n",
    "Ghatering the training examples $\\big(x^{(n)},y^{(n)}\\big)_{1\\le n\\le N}$ into a matrix $X$ and a row vector $Y$:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "| & & |\\\\[-1em]\n",
    "{\\rm x}^{(1)} & \\dots & {\\rm x}^{(N)}\\\\\n",
    "| & & |\\\\\n",
    "\\end{bmatrix}\n",
    "\\qquad\\qquad\n",
    "Y = \\left[ y^{(1)} \\;\\dots\\; y^{(N)} \\right]\n",
    "$$\n",
    "\n",
    "the vectorialized implementation of the forward propagation boils down to \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z^{[1]} &=  W^{[1]} X + {\\rm b}^{[1]}\\\\ \n",
    "A^{[1]} &= \\tanh(Z^{[1]})\\\\\n",
    "Z^{[2]} &=  W^{[2]} A^{[1]} + {\\rm b}^{[2]}\\\\ \n",
    "A^{[2]} &= \\operatorname{sigmoid}(Z^{[2]})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "**Question**: Implement `forward_propagation()`.\n",
    "\n",
    "**Instructions**:\n",
    "- Look above at the mathematical representation of your classifier.\n",
    "- You can use the function `sigmoid()` defined below.\n",
    "- You can use the function `np.tanh()`. It is part of the numpy library.\n",
    "- The steps you have to implement are:\n",
    "    1. Retrieve each parameter from the dictionary \"parameters\" (which is the output of `initialize_parameters()`) by using `parameters[\"..\"]`.\n",
    "    2. Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).\n",
    "- Values needed in the backpropagation are stored in \"`cache`\". The `cache` will be given as an input to the backpropagation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8b6657b56d01cd13c6462d48baf01249",
     "grade": false,
     "grade_id": "sigmoid",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d6a81780b6d7700df7de8e1061bbb017",
     "grade": false,
     "grade_id": "forward_propagation",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    Z1 = None\n",
    "    A1 = None\n",
    "    Z2 = None\n",
    "    A2 = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f7dfada5482e27d7209a4aa78af6b40b",
     "grade": false,
     "grade_id": "forward_propagation_assess",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_assess = np.random.randn(2, 3)\n",
    "\n",
    "parameters = {\n",
    "    'W1': np.array([[-0.00416758, -0.00056267],\n",
    "                    [-0.02136196,  0.01640271],\n",
    "                    [-0.01793436, -0.00841747],\n",
    "                    [ 0.00502881, -0.01245288]]),\n",
    "     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n",
    "     'b1': np.random.randn(4,1),\n",
    "     'b2': np.array([[ -1.3]])\n",
    "}\n",
    "\n",
    "A2, cache = forward_propagation(X_assess, parameters)\n",
    "\n",
    "# Note: we use the mean here just to make sure that your output matches ours. \n",
    "print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <td> 0.262818640198 0.091999045227 -1.30766601287 0.212877681719 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3c559da81d053b1335ce147dd7c133ca",
     "grade": true,
     "grade_id": "forward_propagation_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal( np.mean(cache['Z1']),  0.262818640198)\n",
    "np.testing.assert_almost_equal( np.mean(cache['A1']),  0.091999045227)\n",
    "np.testing.assert_almost_equal( np.mean(cache['Z2']), -1.30766601287)\n",
    "np.testing.assert_almost_equal( np.mean(cache['A2']),  0.212877681719)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Cost function\n",
    "\n",
    "Now that you have computed $A^{[2]}$ (in the Python variable \"`A2`\"), which contains $a^{[2](n)}$ for every example $\\big(x^{(n)},y^{(n)}\\big)$, you can compute the cost function $J$ as follows:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{N} \\sum_{n = 1}^{N} \\Big( \\small y^{(n)}\\log\\left(a^{[2](n)}\\right) + (1-y^{(n)})\\log\\left(1- a^{[2](n)}\\right) \\Big)$$\n",
    "\n",
    "where $\\theta=(W^{[1]},b^{[1]},W^{[2]},b^{[2]})$ contains all the network parametrs.\n",
    "\n",
    "**Exercise**: Implement `compute_cost()` to compute the value of the cost $J$.\n",
    "\n",
    "**Instructions**: There are many ways to implement the cross-entropy loss. For example, you could implement the term $-\\frac{1}{N}\\sum_{n = 1}^{N} \\small y^{(n)}\\log\\left(a^{[2](n)}\\right)$ as follows:\n",
    "\n",
    "```python\n",
    "logprobs = np.multiply(np.log(A2),Y)\n",
    "cost = -np.mean(logprobs)                # no need to use a loop!\n",
    "```\n",
    "\n",
    "Don't forget to add the other term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9476e9817e0ec95ba425433ac9159fad",
     "grade": false,
     "grade_id": "compute_cost",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the cross-entropy cost\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    logprobs = None\n",
    "    cost = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "40b837a029bf4dc09a276b6f42006c0c",
     "grade": false,
     "grade_id": "compute_cost_assess",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "Y_assess = (np.random.randn(1, 3) > 0)\n",
    "parameters = {\n",
    "    'W1': np.array([[-0.00416758, -0.00056267],\n",
    "                    [-0.02136196,  0.01640271],\n",
    "                    [-0.01793436, -0.00841747],\n",
    "                    [ 0.00502881, -0.01245288]]),\n",
    "     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n",
    "     'b1': np.array([[ 0.],[ 0.],[ 0.],[ 0.]]),\n",
    "     'b2': np.array([[ 0.]])\n",
    "}\n",
    "a2 = (np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]))\n",
    "    \n",
    "cost = compute_cost(a2, Y_assess, parameters)\n",
    "\n",
    "print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table style=\"width:20%\">\n",
    "  <tr>\n",
    "    <td>**cost**</td>\n",
    "    <td> 0.693058761... </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2627bb927122ff745e15a08f540b4c0c",
     "grade": true,
     "grade_id": "compute_cost_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(cost, 0.693058761039)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Backward propagation\n",
    "\n",
    "Using the cache computed during forward propagation, you can now implement backward propagation.\n",
    "\n",
    "**Question**: Implement the function `backward_propagation()`.\n",
    "\n",
    "**Instructions**:\n",
    "Backpropagation is usually the hardest (most mathematical) part in deep learning. To help you, you'll find below the six equations describing the vectorialized implementation of backpropagation. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dZ^{[2]} &= A^{[2]} - Y \\\\\n",
    "dZ^{[1]} &= \\big( {W^{[2]}}^\\top dZ^{[2]} \\big) \\circ \\tanh'\\big(Z^{[1]}\\big)\\\\\n",
    "\\\\\n",
    "dW^{[2]} &= \\frac{1}{N} dZ^{[2]} \\, {A^{[1]}}^T \\\\\n",
    "db^{[2]} &= \\frac{1}{N} dZ^{[2]} \\mathbb{1}_N\\\\\n",
    "\\\\\n",
    "dW^{[1]} &= \\frac{1}{N} dZ^{[1]} \\, X^T\\\\\n",
    "db^{[1]} &= \\frac{1}{N} dZ^{[1]} \\mathbb{1}_N\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that $\\mathbb{1}_N=[1\\;1\\;\\dots\\;1]^\\top$, and $\\circ$ denotes the elementwise multiplication.\n",
    "\n",
    "**Tips:**\n",
    " \n",
    " \n",
    " - To compute $dZ^{[1]}$, you'll need to compute $\\tanh'(Z^{[1]})$, that is the derivative of $\\tanh$ in $Z^{[1]})$. It can be shown that, if $a = \\tanh(z)$ then $\\tanh'(z) = 1-a^2$. So you can compute $\\tanh'(Z^{[1]})$ using `(1 - np.power(A1, 2))`.\n",
    " \n",
    " \n",
    " - To compute $db^{[2]}$ and $db^{[1]}$, you may find useful the command `np.mean(..., axis=1, keepdims=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "75d1c9d277e56012c9b77e862b51b766",
     "grade": false,
     "grade_id": "backward_propagation",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: backward_propagation\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    N = X.shape[1] # numer of examples\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    W1 = None\n",
    "    W2 = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A1 = None\n",
    "    A2 = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    ### START CODE HERE ### (≈ 6 lines of code, corresponding to the 6 equations given above)\n",
    "    dZ2 = None\n",
    "    dZ1 = None\n",
    "    dW2 = None\n",
    "    db2 = None\n",
    "    dW1 = None\n",
    "    db1 = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "187b4b91354179e4e428ac6acf5a5c96",
     "grade": false,
     "grade_id": "backward_propagation_assess",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "    \n",
    "X_assess = np.random.randn(2, 3)\n",
    "Y_assess = (np.random.randn(1, 3) > 0)\n",
    "    \n",
    "parameters = {\n",
    "    'W1': np.array([[-0.00416758, -0.00056267],\n",
    "                    [-0.02136196,  0.01640271],\n",
    "                    [-0.01793436, -0.00841747],\n",
    "                    [ 0.00502881, -0.01245288]]),\n",
    "    'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n",
    "    'b1': np.array([[ 0.],\n",
    "                    [ 0.],\n",
    "                    [ 0.],\n",
    "                    [ 0.]]),\n",
    "    'b2': np.array([[ 0.]])\n",
    "}\n",
    "cache = {\n",
    "    'A1': np.array([[-0.00616578,  0.0020626 ,  0.00349619],\n",
    "                    [-0.05225116,  0.02725659, -0.02646251],\n",
    "                    [-0.02009721,  0.0036869 ,  0.02883756],\n",
    "                    [ 0.02152675, -0.01385234,  0.02599885]]),\n",
    "    'A2': np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]),\n",
    "    'Z1': np.array([[-0.00616586,  0.0020626 ,  0.0034962 ],\n",
    "                    [-0.05229879,  0.02726335, -0.02646869],\n",
    "                    [-0.02009991,  0.00368692,  0.02884556],\n",
    "                    [ 0.02153007, -0.01385322,  0.02600471]]),\n",
    "    'Z2': np.array([[ 0.00092281, -0.00056678,  0.00095853]])\n",
    "}\n",
    "\n",
    "grads = backward_propagation(parameters, cache, X_assess, Y_assess)\n",
    "\n",
    "print (\"dW1 =\\n\"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 =\\n\"+ str(grads[\"db1\"]))\n",
    "print (\"dW2 =\\n\"+ str(grads[\"dW2\"]))\n",
    "print (\"db2 =\\n\"+ str(grads[\"db2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "\n",
    "\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td>**dW1**</td>\n",
    "    <td> [[ 0.00301023 -0.00747267]\n",
    " [ 0.00257968 -0.00641288]\n",
    " [-0.00156892  0.003893  ]\n",
    " [-0.00652037  0.01618243]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**db1**</td>\n",
    "    <td>  [[ 0.00176201]\n",
    " [ 0.00150995]\n",
    " [-0.00091736]\n",
    " [-0.00381422]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**dW2**</td>\n",
    "    <td> [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]] </td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  <tr>\n",
    "    <td>**db2**</td>\n",
    "    <td> [[-0.16655712]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "eb8af6120ddd5b2323662f52c930e34e",
     "grade": true,
     "grade_id": "backward_propagation_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "dW1 = grads[\"dW1\"]\n",
    "np.testing.assert_almost_equal(dW1[0,0],  0.00301023)\n",
    "np.testing.assert_almost_equal(dW1[0,1], -0.00747267)\n",
    "np.testing.assert_almost_equal(dW1[1,0],  0.00257968)\n",
    "np.testing.assert_almost_equal(dW1[1,1], -0.00641288)\n",
    "np.testing.assert_almost_equal(dW1[2,0], -0.00156892)\n",
    "np.testing.assert_almost_equal(dW1[2,1],  0.003893)\n",
    "np.testing.assert_almost_equal(dW1[3,0], -0.00652037)\n",
    "np.testing.assert_almost_equal(dW1[3,1],  0.01618243)\n",
    "\n",
    "db1 = grads[\"db1\"]\n",
    "np.testing.assert_almost_equal(db1[0,0],  0.00176201)\n",
    "np.testing.assert_almost_equal(db1[1,0],  0.00150995)\n",
    "np.testing.assert_almost_equal(db1[2,0], -0.00091736)\n",
    "np.testing.assert_almost_equal(db1[3,0], -0.00381422)\n",
    "\n",
    "dW2 = grads[\"dW2\"]\n",
    "np.testing.assert_almost_equal(dW2[0,0],  0.00078841)\n",
    "np.testing.assert_almost_equal(dW2[0,1],  0.01765429)\n",
    "np.testing.assert_almost_equal(dW2[0,2], -0.00084166)\n",
    "np.testing.assert_almost_equal(dW2[0,3], -0.01022527)\n",
    "\n",
    "db2 = grads[\"db2\"]\n",
    "np.testing.assert_almost_equal(db2[0,0], -0.16655712)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Gradient descent\n",
    "\n",
    "**Question**: Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).\n",
    "\n",
    "**General gradient descent rule**: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8d5038e92b54804219ca5744a64b3052",
     "grade": false,
     "grade_id": "update_parameters",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    dW1 = None\n",
    "    db1 = None\n",
    "    dW2 = None\n",
    "    db2 = None\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 \n",
    "    b1 \n",
    "    W2  \n",
    "    b2  \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bfa8a106db696e55e28ccc9321560a81",
     "grade": false,
     "grade_id": "update_parameters_assess",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'W1': np.array([[-0.00615039,  0.0169021 ],\n",
    "                    [-0.02311792,  0.03137121],\n",
    "                    [-0.0169217 , -0.01752545],\n",
    "                    [ 0.00935436, -0.05018221]]),\n",
    "    'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n",
    "    'b1': np.array([[ -8.97523455e-07],\n",
    "                    [  8.15562092e-06],\n",
    "                    [  6.04810633e-07],\n",
    "                    [ -2.54560700e-06]]),\n",
    "    'b2': np.array([[  9.14954378e-05]])\n",
    "}\n",
    "grads = {\n",
    "    'dW1': np.array([[ 0.00023322, -0.00205423],\n",
    "                     [ 0.00082222, -0.00700776],\n",
    "                     [-0.00031831,  0.0028636 ],\n",
    "                     [-0.00092857,  0.00809933]]),\n",
    "    'dW2': np.array([[ -1.75740039e-05,   3.70231337e-03,  -1.25683095e-03, -2.55715317e-03]]),\n",
    "    'db1': np.array([[  1.05570087e-07],\n",
    "                     [ -3.81814487e-06],\n",
    "                     [ -1.90155145e-07],\n",
    "                     [  5.46467802e-07]]),\n",
    "    'db2': np.array([[ -1.08923140e-05]])}\n",
    "\n",
    "parameters = update_parameters(parameters, grads)\n",
    "\n",
    "print(\"W1 =\\n\" + str(parameters[\"W1\"]))\n",
    "print(\"b1 =\\n\" + str(parameters[\"b1\"]))\n",
    "print(\"W2 =\\n\" + str(parameters[\"W2\"]))\n",
    "print(\"b2 =\\n\" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td>**W1**</td>\n",
    "    <td> [[-0.00643025  0.01936718]\n",
    " [-0.02410458  0.03978052]\n",
    " [-0.01653973 -0.02096177]\n",
    " [ 0.01046864 -0.05990141]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1**</td>\n",
    "    <td> [[ -1.02420756e-06]\n",
    " [  1.27373948e-05]\n",
    " [  8.32996807e-07]\n",
    " [ -3.20136836e-06]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[-0.01041081 -0.04463285  0.01758031  0.04747113]] </td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  <tr>\n",
    "    <td>**b2**</td>\n",
    "    <td> [[ 0.00010457]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f5bb659897b2fd6507c4e86c385793b8",
     "grade": true,
     "grade_id": "update_parameters_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "W1 = parameters[\"W1\"]\n",
    "np.testing.assert_almost_equal(W1[0,0], -0.00643025)\n",
    "np.testing.assert_almost_equal(W1[0,1],  0.01936718)\n",
    "np.testing.assert_almost_equal(W1[1,0], -0.02410458)\n",
    "np.testing.assert_almost_equal(W1[1,1],  0.03978052)\n",
    "np.testing.assert_almost_equal(W1[2,0], -0.01653973)\n",
    "np.testing.assert_almost_equal(W1[2,1], -0.02096177)\n",
    "np.testing.assert_almost_equal(W1[3,0],  0.01046864)\n",
    "np.testing.assert_almost_equal(W1[3,1], -0.05990141)\n",
    "\n",
    "b1 = parameters[\"b1\"]\n",
    "np.testing.assert_almost_equal(b1[0,0], -1.02420756e-06)\n",
    "np.testing.assert_almost_equal(b1[1,0],  1.27373948e-05)\n",
    "np.testing.assert_almost_equal(b1[2,0],  8.32996807e-07)\n",
    "np.testing.assert_almost_equal(b1[3,0], -3.20136836e-06)\n",
    "\n",
    "W2 = parameters[\"W2\"]\n",
    "np.testing.assert_almost_equal(W2[0,0], -0.01041081)\n",
    "np.testing.assert_almost_equal(W2[0,1], -0.04463285)\n",
    "np.testing.assert_almost_equal(W2[0,2],  0.01758031)\n",
    "np.testing.assert_almost_equal(W2[0,3],  0.04747113)\n",
    "\n",
    "b2 = parameters[\"b2\"]\n",
    "np.testing.assert_almost_equal(b2[0,0], 0.00010457)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - Integrate all the pieces in nn_model() ####\n",
    "\n",
    "**Question**: Build your neural network model in `nn_model()`.\n",
    "\n",
    "**Instructions**: The neural network model has to use the previous functions in the right order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8608d3478f9fe1d1d482de165e210bc0",
     "grade": false,
     "grade_id": "nn_model",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: nn_model\n",
    "\n",
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
    "    ### START CODE HERE ### (≈ 5 lines of code)\n",
    "    parameters = None\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        #A2, cache = None\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        #cost = None\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        #grads = None\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        #parameters = None\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ca24c249caab343fb201ecdc238e62a2",
     "grade": false,
     "grade_id": "nn_model_assess",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_assess = np.random.randn(2, 3)\n",
    "Y_assess = (np.random.randn(1, 3) > 0)\n",
    "\n",
    "parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=True)\n",
    "\n",
    "print(\"W1 =\\n\" + str(parameters[\"W1\"]))\n",
    "print(\"b1 =\\n\" + str(parameters[\"b1\"]))\n",
    "print(\"W2 =\\n\" + str(parameters[\"W2\"]))\n",
    "print(\"b2 =\\n\" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:90%\">\n",
    "\n",
    "<tr> \n",
    "    <td> \n",
    "        **cost after iteration 0**\n",
    "    </td>\n",
    "    <td> \n",
    "        0.692739\n",
    "    </td>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "    <td> \n",
    "        <center> $\\vdots$ </center>\n",
    "    </td>\n",
    "    <td> \n",
    "        <center> $\\vdots$ </center>\n",
    "    </td>\n",
    "</tr>\n",
    "\n",
    "  <tr>\n",
    "    <td>**W1**</td>\n",
    "    <td> [[-0.65848169  1.21866811]\n",
    " [-0.76204273  1.39377573]\n",
    " [ 0.5792005  -1.10397703]\n",
    " [ 0.76773391 -1.41477129]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1**</td>\n",
    "    <td> [[ 0.287592  ]\n",
    " [ 0.3511264 ]\n",
    " [-0.2431246 ]\n",
    " [-0.35772805]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[-2.45566237 -3.27042274  2.00784958  3.36773273]] </td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  <tr>\n",
    "    <td>**b2**</td>\n",
    "    <td> [[ 0.20459656]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "26de37d28db9a860f536e7723f26b228",
     "grade": true,
     "grade_id": "nn_model_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "W1 = parameters[\"W1\"]\n",
    "np.testing.assert_almost_equal(W1[0,0], -0.65848169)\n",
    "np.testing.assert_almost_equal(W1[0,1],  1.21866811)\n",
    "np.testing.assert_almost_equal(W1[1,0], -0.76204273)\n",
    "np.testing.assert_almost_equal(W1[1,1],  1.39377573)\n",
    "np.testing.assert_almost_equal(W1[2,0],  0.5792005)\n",
    "np.testing.assert_almost_equal(W1[2,1], -1.10397703)\n",
    "np.testing.assert_almost_equal(W1[3,0],  0.76773391)\n",
    "np.testing.assert_almost_equal(W1[3,1], -1.41477129)\n",
    "\n",
    "b1 = parameters[\"b1\"]\n",
    "np.testing.assert_almost_equal(b1[0,0],  0.287592)\n",
    "np.testing.assert_almost_equal(b1[1,0],  0.3511264)\n",
    "np.testing.assert_almost_equal(b1[2,0], -0.2431246)\n",
    "np.testing.assert_almost_equal(b1[3,0], -0.35772805)\n",
    "\n",
    "W2 = parameters[\"W2\"]\n",
    "np.testing.assert_almost_equal(W2[0,0], -2.45566237)\n",
    "np.testing.assert_almost_equal(W2[0,1], -3.27042274)\n",
    "np.testing.assert_almost_equal(W2[0,2],  2.00784958)\n",
    "np.testing.assert_almost_equal(W2[0,3],  3.36773273)\n",
    "\n",
    "b2 = parameters[\"b2\"]\n",
    "np.testing.assert_almost_equal(b2[0,0], 0.20459656)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 - Predictions\n",
    "\n",
    "**Question**: Use your model to predict by building predict().\n",
    "Use forward propagation to predict results.\n",
    "\n",
    "**Reminder**: predictions = $y_{prediction} = \\mathbb 1 \\text{{activation > 0.5}} = \\begin{cases}\n",
    "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$  \n",
    "    \n",
    "As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: ```X_new = (X > threshold)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0958ad4f0bbf52438b133606c112477b",
     "grade": false,
     "grade_id": "predict",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    #A2, cache = None\n",
    "    #predictions = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3fda8f582b5532b4d12b91c4159e5154",
     "grade": false,
     "grade_id": "predict_assess",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_assess = np.random.randn(2, 3)\n",
    "parameters = {\n",
    "    'W1': np.array([[-0.00615039,  0.0169021 ],\n",
    "                    [-0.02311792,  0.03137121],\n",
    "                    [-0.0169217 , -0.01752545],\n",
    "                    [ 0.00935436, -0.05018221]]),\n",
    "    'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n",
    "    'b1': np.array([[ -8.97523455e-07],\n",
    "                    [  8.15562092e-06],\n",
    "                    [  6.04810633e-07],\n",
    "                    [ -2.54560700e-06]]),\n",
    "     'b2': np.array([[  9.14954378e-05]])\n",
    "}\n",
    "\n",
    "predictions = predict(parameters, X_assess)\n",
    "\n",
    "print(\"predictions mean = \" + str(np.mean(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "\n",
    "<table style=\"width:40%\">\n",
    "  <tr>\n",
    "    <td>**predictions mean**</td>\n",
    "    <td> 0.666666666667 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3880ae75d6f2083324823345e5eebef3",
     "grade": true,
     "grade_id": "predict_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(np.mean(predictions), 0.666666666667)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on the dataset\n",
    "\n",
    "It is time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of $n_h$ hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:40%\">\n",
    "  <tr>\n",
    "    <td>**Cost after iteration 9000**</td>\n",
    "    <td> 0.218607 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    \n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    h = 0.01\n",
    "    \n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict the function value for the whole grid\n",
    "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.5)\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[0, :], X[1, :], c=y.flatten(), cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
    "plt.title(\"Decision Boundary for hidden layer size \" + str(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy\n",
    "predictions = predict(parameters, X)\n",
    "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:15%\">\n",
    "  <tr>\n",
    "    <td>**Accuracy**</td>\n",
    "    <td> 90% </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is really high, as the model has learnt the leaf patterns of the flower! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression. \n",
    "\n",
    "Now, let's try out several hidden layer sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hidden layer size\n",
    "\n",
    "Run the following code. It may take 1-2 minutes. You will observe different behaviors of the model for various hidden layer sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take about 2 minutes to run\n",
    "\n",
    "plt.figure(figsize=(16, 32))\n",
    "hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\n",
    "for i, n_h in enumerate(hidden_layer_sizes):\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    plt.title('Hidden Layer of size %d' % n_h)\n",
    "    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n",
    "    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
    "    predictions = predict(parameters, X)\n",
    "    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
    "    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "- The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data. \n",
    "- The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to  fits the data well without also incurring noticable overfitting.\n",
    "- You will also learn later about regularization, which lets you use very large models (such as n_h = 50) without much overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You've learnt to:**\n",
    "- Build a complete neural network with a hidden layer\n",
    "- Implement forward propagation and backpropagation, and trained a neural network\n",
    "- See the impact of varying the hidden layer size, including overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further reading:**\n",
    "- http://scs.ryerson.ca/~aharley/neural-networks/\n",
    "- http://cs231n.github.io/neural-networks-case-study/\n",
    "\n",
    "**Credits:** \n",
    "- This assignment is partly based on Andrew Ng's course on Coursera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
